{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89083c0d-17e1-4fbd-8045-43b4365a08a3",
   "metadata": {},
   "source": [
    "Data design:\n",
    "This code is for basic data design and cleaning. The objective is to generate three tables from the raw full dataset-JSON data. \n",
    "\n",
    "We plan to generate three tables from the raw JSON data:\n",
    "\n",
    "1. Researcher table: researcher_df \n",
    "This table contains one row per researcher.\n",
    "Required fields include: \n",
    "\"researcher_id\": a unique identifier for each researcher; \n",
    "\"first_name\"; \n",
    "\"last_name\"; \n",
    "\"email\"; \n",
    "\"title\"; \n",
    "\"overview\":\n",
    "\"department_raw\" (parsed from the title directly before cleaning); \n",
    "\"role\": the role of this research in Duke University(for example, professor, assistant professor, graduate student, etc.)The role field for each researcher is determined through a two-stage extraction pipeline. The general principle is: use the primaryAppointment.title first; if that fails, fall back to parsing the overview text. All matching is case-insensitive unless otherwise specified.\n",
    "pub_count: the overall amounts of publication of this researcher;\n",
    "latest_pub_date: the newest publication date.\n",
    "\n",
    "\n",
    "2. Publication table: publication_clean_df \n",
    "****** There are a total of 481,572 author–publication rows, but the number of orginal pub_id(pulled out directly from duke database)'s  values is also 481,572. This means that each pub_id appears only once, and there is no case where two authors share the same pub_id. Therefore, pub_id cannot be used as an indicator of co-authorship, because it does not represent shared publications among multiple researchers. \n",
    "\n",
    "****** In this case, use \"title to create unique_ids and coauthor pairs. (After testing, there is no missing in titles)\n",
    "\n",
    "this table contains one row per publication.\n",
    "Required fields include:\n",
    "\n",
    "\"pub_unique_id\": the unique_id we created by titles. \n",
    "\"researcher_id\": researcher_id,\n",
    "\"researcher_full_name\": first_name,\n",
    "\"pub_title\": pub_title,\n",
    "\"pub_abstract\": pub_abstract,\n",
    "\"pub_doi\": pub_doi,\n",
    "\"duke_authors\": All Duke-affiliated authors matched from the Duke researcher database.\n",
    "\"raw_pub_ids\": All original pub_id values associated with this publication in the Duke source data.\n",
    "\"all_authors\": all_authors, including people who are not matched via Duke database,\n",
    "\"pub_date\": the date this research was published. \n",
    "\n",
    "\n",
    "3. Coauthor relationship: coauthor_pairs_final  \n",
    "This table contains one row per co-author pair. \n",
    "Based on the unique_ids we used \"title\" to create, we created the deduplicated author.\n",
    "Required fields include:\n",
    "Researcher_A_id,\n",
    "Researcher_B_id,\n",
    "Researcher_A_name,\n",
    "Researcher_B_name,\n",
    "joint_pub_numbers: how many publications they collaborated. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7ad6a8e-9cc6-4912-8f39-9fb2be5b3bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nanoid in c:\\users\\alejandro\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import requests\n",
    "import os\n",
    "!pip install nanoid\n",
    "from nanoid import generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f432bab4-2b96-4bfb-aa30-fb52bbd41890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "import json\n",
    "\n",
    "with open(\"people_2025-11-25_13-57-52.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data_dict = json.load(f)\n",
    "data = data_dict[\"results\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "626feb98-271c-4cb6-bfca-91939d98b954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[list of length 11552]\n",
      "  currentResearch: (list)\n",
      "    [list of length 1]\n",
      "      id: (str)\n",
      "        38405 (str)\n",
      "  email: (str)\n",
      "    aaliya.aaliya@duke.edu (str)\n",
      "  firstName: (str)\n",
      "    Aaliya (str)\n",
      "  lastName: (str)\n",
      "    Aaliya (str)\n",
      "  overview: (str)\n",
      "    Aaliya is a Ph.D. Candidate working under the supervision of Prof. Dr. Brian McAdoo in PlanetLab in the Division of Earth and Climate Sciences at Duke University&rsquo;s Nicholas School of the Environment. Her doctoral research focuses on understanding the planetary-scale changes such as climate change impacting the health and well-being of rural mountain communities in the Hindu Kush Himalayan (HKH) Region, focusing specifically on Nepal and Pakistan. (str)\n",
      "  primaryAppointment: (NoneType)\n",
      "    None (NoneType)\n",
      "  publications: (dict)\n",
      "    count: (int)\n",
      "      0 (int)\n",
      "    results: (list)\n",
      "      [list of length 0]\n"
     ]
    }
   ],
   "source": [
    "# data strcuture\n",
    "def print_json_structure(d, indent=0):\n",
    "    prefix = \"  \" * indent\n",
    "    if isinstance(d, dict):\n",
    "        for k, v in d.items():\n",
    "            print(f\"{prefix}{k}: ({type(v).__name__})\")\n",
    "            print_json_structure(v, indent + 1)\n",
    "    elif isinstance(d, list):\n",
    "        print(f\"{prefix}[list of length {len(d)}]\")\n",
    "        if len(d) > 0:\n",
    "            print_json_structure(d[0], indent + 1)\n",
    "    else:\n",
    "        # primitive value -> 不展开\n",
    "        print(f\"{prefix}{d} ({type(d).__name__})\")\n",
    "\n",
    "print_json_structure(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7dd2ffc5-aecd-4064-9503-2ab2b3fb39e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': '38405'}]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[{'id': '11060'}]\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# need to figure out what the 'currentResearch' refers to.\n",
    "for i in range(10):\n",
    "    print(data[i]['currentResearch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00895b2d-7221-4654-87de-41830af69027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'currentResearch': [],\n",
       " 'email': 'bijan.abar@duke.edu',\n",
       " 'firstName': 'Bijan',\n",
       " 'lastName': 'Abar',\n",
       " 'overview': None,\n",
       " 'primaryAppointment': None,\n",
       " 'publications': {'count': 1,\n",
       "  'results': [{'id': '0712180-1640965-8',\n",
       "    'publication': {'abstract': 'BACKGROUND: Collapse of the talus and peri-talar arthritis pose treatment challenges due to the anatomy and location of the talus as a keystone of the foot and ankle. Custom 3D-printed total talus replacement (TTR) and combined total ankle total talus replacement (TATTR) have emerged as treatment options for these pathologies. However, the safety and efficacy of these implants is unknown due to the limited number of cases and short follow-up durations. METHODS: This was a retrospective study to assess surgical outcomes of patients who underwent a TTR and TATTR with or without subtalar fusion. Patient demographics, intraoperative parameters, device related surgical and non-surgical events, imaging and clinical evaluations, and patient reported outcome (PRO) measures were compiled. RESULTS: A total of 38 patients received a custom 3D-printed implant with mean follow-up time of 22.1 (range: 12-45) months. In this cohort, 7 (18.4\\xa0%) required secondary surgery and 3 (7.9\\xa0%) required implant removal. Multivariate logistic regression revealed that patient diagnosis of depression was a significant predictor of secondary surgery with an OR 17.50 (p\\xa0=\\xa00.037). Significant postoperative improvements were observed in the talocalcaneal height (p\\xa0=\\xa00.005) and talar declination angle (p\\xa0=\\xa00.013) for the TATTR group. VAS and PROMIS pain interference (PI) scores demonstrated an initial significant improvement in pain, but this improvement did not maintain significance at most recent follow-up. However, there was a significant increase in the PROMIS physical function (PF) scores (p\\xa0=\\xa00.037) at most recent follow-up. CONCLUSION: These results demonstrate that TTR and TATTR provide significant improvement in post-operative radiographic foot and ankle alignment and physical function at the two-year timepoint. PRO findings suggest that patients are more active after surgery. Surgeons considering proceeding with either of these procedures should counsel patients about pain and functional outcomes as well as realistic expectations in patients with depression. LEVEL OF EVIDENCE: Level 3.',\n",
       "     'allAuthors': {'fullList': 'Mitra, K; Anastasio, AT; Wu, KA; Abar, B; Schweitzer, KM; Parekh, SG; Easley, ME; Adams, SB'},\n",
       "     'doi': '10.1016/j.fas.2024.07.011',\n",
       "     'publicationDate': {'date': '2025-02-01'},\n",
       "     'title': 'Outcomes of cobalt-chrome 3D-printed total talus replacement with and without combined total ankle replacement.'}}]}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54f2c2dd-54a9-4b9e-a383-d32979c17503",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 11552\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(9999)\n",
    "\n",
    "def stable_id():\n",
    "    return hex(random.getrandbits(64))[2:]\n",
    "\n",
    "# data is already a list of researchers\n",
    "print(type(data), len(data))  # just to confirm\n",
    "\n",
    "for person in data:\n",
    "    person[\"unique_id\"] = stable_id()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0ae9a963-a6c4-432b-9b3e-6a1dc80a5877",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Role Extraction Logic \n",
    "\n",
    "The role field for each researcher is determined through a two-stage extraction pipeline. The general principle is:\n",
    "use the primaryAppointment.title first; if that fails, fall back to parsing the overview text.\n",
    "All matching is case-insensitive unless otherwise specified.\n",
    "    1. Extraction from primaryAppointment.title (Primary Source)\n",
    "\n",
    "If a researcher has a non-empty primaryAppointment.title, this field is treated as the most reliable indicator of academic rank or position.\n",
    "The title is normalized to lowercase, and the following ordered rules are applied:\n",
    "\n",
    "(1) Emeritus/Emerita detection (highest priority)\n",
    "\n",
    "If the title contains “emeritus” or “emerita”, the role is assigned accordingly (e.g., \"Professor Emeritus\").\n",
    "\n",
    "(2) Visiting appointments\n",
    "\n",
    "If the title includes “visiting”, the role becomes either:\n",
    "\n",
    "\"Visiting Professor\" (if “professor” also appears), or\n",
    "\n",
    "\"Visiting Faculty\" (otherwise).\n",
    "\n",
    "(3) In Residence roles\n",
    "\n",
    "Titles containing “in residence” are categorized based on preceding keywords such as:\n",
    "\n",
    "\"Executive in Residence\"\n",
    "\n",
    "\"Artist in Residence\"\n",
    "\n",
    "\"Scholar in Residence\"\n",
    "\n",
    "(4) Standard faculty ranks\n",
    "\n",
    "Roles are assigned based on the presence of specific rank descriptors:\n",
    "\n",
    "\"Associate Professor\"\n",
    "\n",
    "\"Assistant Professor\"\n",
    "\n",
    "\"Professor\"\n",
    "\n",
    "\"Clinical Professor\" / \"Clinical Faculty\"\n",
    "\n",
    "\"Research Professor\"\n",
    "\n",
    "\"Adjunct Professor\"\n",
    "The matching is done in a prioritized order (associate > assistant > professor, etc.).\n",
    "\n",
    "(5) Lecturer / Instructor / Fellow\n",
    "\n",
    "If none of the above apply, roles may be assigned based on:\n",
    "\n",
    "\"Lecturer\" or \"Lecturing Fellow\"\n",
    "\n",
    "\"Instructor\"\n",
    "\n",
    "(6) Fallback\n",
    "\n",
    "If no rule matches, the role remains None and the overview text is used in the next stage.\n",
    "\n",
    "2. Extraction from overview (Fallback Source)\n",
    "\n",
    "If the role is still None after title-based matching, the researcher’s overview text is parsed using case-insensitive regular expressions. The extraction follows a priority order to prevent overly broad matches (such as “student”) from overriding more specific roles.\n",
    "\n",
    "Priority order for overview-based extraction:\n",
    "(1) PhD-level graduate roles\n",
    "\n",
    "Detected via patterns such as:\n",
    "\n",
    "\"Ph.D. Candidate\"\n",
    "\n",
    "\"PhD Candidate\"\n",
    "\n",
    "\"PhD Student\"\n",
    "\n",
    "\"Doctoral Student\"\n",
    "These all map to:\n",
    "\"PhD Candidate\"\n",
    "\n",
    "(2) Graduate Student\n",
    "\n",
    "Matches \"Graduate Student\" → \"Graduate Student\"\n",
    "\n",
    "(3) Director (capital D only)\n",
    "\n",
    "To avoid false positives, only \"Director\" with capital D is matched → \"Director\"\n",
    "\n",
    "(4) Student (lowest priority)\n",
    "\n",
    "As the final fallback, occurrences of \"student\" assign:\"Student\"\n",
    "\n",
    "This rule is only evaluated if none of the more specific patterns match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f99d8144-da22-4be2-a8b1-11c2ffe5fa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# role extraction from \"PrimaryAppionment[title]\n",
    "\n",
    "import re\n",
    "\n",
    "def get_role_phrase(raw_title: str):\n",
    "    \"\"\"Extract the role phrase before 'in/of/at' or comma.\"\"\"\n",
    "    if not raw_title:\n",
    "        return None\n",
    "    t = raw_title.strip()\n",
    "    m = re.match(r'^(.*?)(?:\\s+(?:in|of|at)\\b|,)', t, flags=re.I)\n",
    "    if m:\n",
    "        return m.group(1).strip()\n",
    "    return t\n",
    "\n",
    "\n",
    "def normalize_role(raw_title: str):\n",
    "    \"\"\"Normalize academic role titles into a standard label.\"\"\"\n",
    "    if not raw_title:\n",
    "        return None\n",
    "    \n",
    "    # Step 1: extract core role phrase\n",
    "    t = get_role_phrase(raw_title)\n",
    "    if not t:\n",
    "        return None\n",
    "    s = t.strip()\n",
    "\n",
    "    # --- Matching rules in priority order ---\n",
    "\n",
    "    # Emeritus/Emerita\n",
    "    if re.search(r'\\bProfessor\\b.*\\bEmerit', s, flags=re.I):\n",
    "        return 'Professor Emeritus'\n",
    "\n",
    "    # Professor of the Practice\n",
    "    if re.search(r'\\bProfessor\\s+of\\s+the\\s+Practice\\b', s, flags=re.I):\n",
    "        return 'Professor of the Practice'\n",
    "    if re.search(r'\\bAssociate\\s+Professor\\s+of\\s+the\\s+Practice\\b', s, flags=re.I):\n",
    "        return 'Associate Professor of the Practice'\n",
    "    if re.search(r'\\bAssistant\\s+Professor\\s+of\\s+the\\s+Practice\\b', s, flags=re.I):\n",
    "        return 'Assistant Professor of the Practice'\n",
    "\n",
    "    # Special prefixes\n",
    "    if re.search(r'\\bAdjunct\\s+Professor\\b', s, flags=re.I):\n",
    "        return 'Adjunct Professor'\n",
    "    if re.search(r'\\bClinical\\s+Professor\\b', s, flags=re.I):\n",
    "        return 'Clinical Professor'\n",
    "    if re.search(r'\\bResearch\\s+Professor\\b', s, flags=re.I):\n",
    "        return 'Research Professor'\n",
    "    if re.search(r'\\bVisiting\\s+Professor\\b', s, flags=re.I):\n",
    "        return 'Visiting Professor'\n",
    "    \n",
    "    # Medical titles\n",
    "    if re.search(r'\\bMedical\\s+Assistant\\s+Professor\\b', s, flags=re.I):\n",
    "        return 'Medical Assistant Professor'\n",
    "    if re.search(r'\\bMedical\\s+Associate\\s+Professor\\b', s, flags=re.I):\n",
    "        return 'Medical Associate Professor'\n",
    "    if re.search(r'\\bMedical\\s+Professor\\b', s, flags=re.I):\n",
    "        return 'Medical Professor'\n",
    "\n",
    "    # Standard professor ranks\n",
    "    if re.search(r'\\bAssociate\\s+Professor\\b', s, flags=re.I):\n",
    "        return 'Associate Professor'\n",
    "    if re.search(r'\\bAssistant\\s+Professor\\b', s, flags=re.I):\n",
    "        return 'Assistant Professor'\n",
    "    if re.search(r'\\bProfessor\\b', s, flags=re.I):\n",
    "        return 'Professor'\n",
    "\n",
    "    # Clinical roles\n",
    "    if re.search(r'\\bClinical\\s+Faculty\\b', s, flags=re.I):\n",
    "        return 'Clinical Faculty'\n",
    "    if re.search(r'\\bAssociate\\s+Clinical\\s+Professor\\b', s, flags=re.I):\n",
    "        return 'Associate Clinical Professor'\n",
    "    if re.search(r'\\bAssistant\\s+Clinical\\s+Professor\\b', s, flags=re.I):\n",
    "        return 'Assistant Clinical Professor'\n",
    "    if re.search(r'\\bClinical\\s+Associate\\b', s, flags=re.I):\n",
    "        return 'Clinical Associate'\n",
    "    if re.search(r'\\bClinical\\s+Affiliate\\b', s, flags=re.I):\n",
    "        return 'Clinical Affiliate'\n",
    "\n",
    "    # Lecturer / Fellow\n",
    "    if re.search(r'\\bSenior\\s+Lecturing\\s+Fellow\\b', s, flags=re.I):\n",
    "        return 'Senior Lecturing Fellow'\n",
    "    if re.search(r'\\bLecturing\\s+Fellow\\b', s, flags=re.I):\n",
    "        return 'Lecturing Fellow'\n",
    "    if re.search(r'\\bSenior\\s+Lecturer\\b', s, flags=re.I):\n",
    "        return 'Senior Lecturer'\n",
    "    if re.search(r'\\bLecturer\\b', s, flags=re.I):\n",
    "        return 'Lecturer'\n",
    "\n",
    "    # Instructor family\n",
    "    if re.search(r'\\bMedical\\s+Instructor\\b', s, flags=re.I):\n",
    "        return 'Medical Instructor'\n",
    "    if re.search(r'\\bInstructor\\b', s, flags=re.I):\n",
    "        return 'Instructor'\n",
    "\n",
    "    # In Residence / Artist in Residence / Scholar in Residence\n",
    "    if re.search(r'\\bExecutive\\s+In\\s+Residence\\b', s, flags=re.I):\n",
    "        return 'Executive in Residence'\n",
    "    if re.search(r'\\bArtist\\s+In\\s+Residence\\b', s, flags=re.I):\n",
    "        return 'Artist in Residence'\n",
    "    if re.search(r'\\bScholar\\s+In\\s+Residence\\b', s, flags=re.I):\n",
    "        return 'Scholar in Residence'\n",
    "\n",
    "    # Fallback: return original phrase\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa64ea04-e47f-4fa0-b554-2d8bb7d847df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: extract title from primaryAppointment\n",
    "def extract_pa_title(pa):\n",
    "    if isinstance(pa, dict):\n",
    "        return pa.get(\"title\")\n",
    "    return None\n",
    "\n",
    "# Step 2: add the new field to each researcher dict\n",
    "for person in data:   # data is your full JSON list\n",
    "    pa_title = extract_pa_title(person.get(\"primaryAppointment\"))\n",
    "    role = normalize_role(pa_title)  # use the function we defined earlier\n",
    "    person[\"role\"] = role\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5be5b3f0-0f41-4d6d-9fe1-941ec7486a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of roles filled from overview: 368\n"
     ]
    }
   ],
   "source": [
    "# if the role is null, continue to fill the role with \"overview\"\n",
    "\n",
    "import re\n",
    "\n",
    "# 1. Regex pattern for roles in overview (case-insensitive)\n",
    "role_pattern_overview = re.compile(\n",
    "    r\"(\"\n",
    "    r\"ph\\.?\\s*d\\.?\\s*candidate|\"        # PhD Candidate / Ph.D. Candidate\n",
    "    r\"ph\\.?\\s*d\\.?\\s*student|\"          # PhD Student / Ph.D. Student\n",
    "    r\"doctoral\\s+student|\"                # Doctoral Student\n",
    "    r\"master'?s?\\s+student|\"              # master's student / masters student\n",
    "    r\"m\\.?\\s*s\\.?\\s*student|\"           # M.S. student / MS student\n",
    "    r\"master\\s+student|\"                  # Master student\n",
    "    r\"graduate\\s+student|\"                # Graduate student\n",
    "    r\"\\bdirector\\b\"                      # Director\n",
    "    r\")\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "# 2. Fill role from overview ONLY when existing role is None\n",
    "filled_from_overview = 0\n",
    "\n",
    "for person in data:\n",
    "    # Only try overview if role is currently None\n",
    "    if person.get(\"role\") is None:\n",
    "        overview = person.get(\"overview\") or \"\"\n",
    "        \n",
    "        # Search in overview text\n",
    "        match = role_pattern_overview.search(overview)\n",
    "        if match:\n",
    "            # Use the matched string as role (keep original casing from overview)\n",
    "            person[\"role\"] = match.group(0).strip()\n",
    "            filled_from_overview += 1\n",
    "\n",
    "print(\"Number of roles filled from overview:\", filled_from_overview)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2fb3d3f5-5bea-4e43-ba4c-d9f580c0efd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# department/major \n",
    "import re\n",
    "\n",
    "def extract_department_from_title(primary_appointment):\n",
    "    \"\"\"\n",
    "    primary_appointment is either:\n",
    "      - None\n",
    "      - dict like {\"title\": \"Assistant Professor in ...\"}\n",
    "    \"\"\"\n",
    "    # 1. If no primaryAppointment\n",
    "    if primary_appointment is None:\n",
    "        return None\n",
    "    \n",
    "    # 2. Extract string from {\"title\": \"...\"} \n",
    "    if isinstance(primary_appointment, dict):\n",
    "        title = primary_appointment.get(\"title\", None)\n",
    "    else:\n",
    "        # fallback: if it's already a string\n",
    "        title = primary_appointment\n",
    "    \n",
    "    if not isinstance(title, str):\n",
    "        return None\n",
    "    \n",
    "    t = title.strip()\n",
    "    if not t:\n",
    "        return None\n",
    "\n",
    "    lower = t.lower()\n",
    "\n",
    "    # --- extraction rules ---\n",
    "    idx_in = lower.rfind(\" in \")\n",
    "    if idx_in != -1:\n",
    "        dept = t[idx_in + len(\" in \"):].strip()\n",
    "    else:\n",
    "        idx_of = lower.rfind(\" of \")\n",
    "        if idx_of != -1:\n",
    "            dept = t[idx_of + len(\" of \"):].strip()\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    # remove \"the \"\n",
    "    dept = re.sub(r\"^\\s*the\\s+\", \"\", dept, flags=re.IGNORECASE)\n",
    "\n",
    "    # remove prefixes\n",
    "    dept = re.sub(r\"^(Department|Division|School|Program|Centre|Center)\\s+of\\s+\", \"\", dept, flags=re.IGNORECASE).strip()\n",
    "\n",
    "    # Handle \"Department of X\" inside string\n",
    "    m = re.search(r\"Department\\s+of\\s+(.+)\", dept, flags=re.IGNORECASE)\n",
    "    if m:\n",
    "        dept = m.group(1).strip()\n",
    "\n",
    "    # Handle \"School of Nursing\" → \"Nursing\"\n",
    "    m2 = re.search(r\"School\\s+of\\s+(.+)\", dept, flags=re.IGNORECASE)\n",
    "    if m2:\n",
    "        dept = m2.group(1).strip()\n",
    "\n",
    "    return dept if dept else None\n",
    "\n",
    "\n",
    "# --- Attach to your data ---\n",
    "for person in data:\n",
    "    person[\"department_raw\"] = extract_department_from_title(person.get(\"primaryAppointment\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac34bdde-d2fd-4a09-b460-3b384b23e0e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>researcher_id</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>full_name</th>\n",
       "      <th>email</th>\n",
       "      <th>title</th>\n",
       "      <th>department_raw</th>\n",
       "      <th>role</th>\n",
       "      <th>overview</th>\n",
       "      <th>pub_count</th>\n",
       "      <th>latest_pub_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1fc9350ad5b2af40</td>\n",
       "      <td>Aaliya</td>\n",
       "      <td>Aaliya</td>\n",
       "      <td>Aaliya Aaliya</td>\n",
       "      <td>aaliya.aaliya@duke.edu</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Ph.D. Candidate</td>\n",
       "      <td>Aaliya is a Ph.D. Candidate working under the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c0fa5e68285a665a</td>\n",
       "      <td>Ehsan</td>\n",
       "      <td>Abadi</td>\n",
       "      <td>Ehsan Abadi</td>\n",
       "      <td>ehsan.abadi@duke.edu</td>\n",
       "      <td>Associate Professor in Radiology</td>\n",
       "      <td>Radiology</td>\n",
       "      <td>Associate Professor</td>\n",
       "      <td>&lt;p&gt;&lt;strong&gt;Ehsan Abadi, PhD&lt;/strong&gt; is an ima...</td>\n",
       "      <td>170</td>\n",
       "      <td>2025-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>218eb826135ae9b2</td>\n",
       "      <td>Bijan</td>\n",
       "      <td>Abar</td>\n",
       "      <td>Bijan Abar</td>\n",
       "      <td>bijan.abar@duke.edu</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31ccfa85ca721145</td>\n",
       "      <td>Aamna</td>\n",
       "      <td>Abbasi</td>\n",
       "      <td>Aamna Abbasi</td>\n",
       "      <td>aamna.abbasi@duke.edu</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>f3e7e274490fb365</td>\n",
       "      <td>James</td>\n",
       "      <td>Abbruzzese</td>\n",
       "      <td>James Abbruzzese</td>\n",
       "      <td>james.abbruzzese@duke.edu</td>\n",
       "      <td>Professor Emeritus of Medicine</td>\n",
       "      <td>Medicine</td>\n",
       "      <td>Professor Emeritus</td>\n",
       "      <td>My research interests include the clinical stu...</td>\n",
       "      <td>585</td>\n",
       "      <td>2025-02-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      researcher_id first_name   last_name         full_name  \\\n",
       "0  1fc9350ad5b2af40     Aaliya      Aaliya     Aaliya Aaliya   \n",
       "1  c0fa5e68285a665a      Ehsan       Abadi       Ehsan Abadi   \n",
       "2  218eb826135ae9b2      Bijan        Abar        Bijan Abar   \n",
       "3  31ccfa85ca721145      Aamna      Abbasi      Aamna Abbasi   \n",
       "4  f3e7e274490fb365      James  Abbruzzese  James Abbruzzese   \n",
       "\n",
       "                       email                             title department_raw  \\\n",
       "0     aaliya.aaliya@duke.edu                              None           None   \n",
       "1       ehsan.abadi@duke.edu  Associate Professor in Radiology      Radiology   \n",
       "2        bijan.abar@duke.edu                              None           None   \n",
       "3      aamna.abbasi@duke.edu                              None           None   \n",
       "4  james.abbruzzese@duke.edu    Professor Emeritus of Medicine       Medicine   \n",
       "\n",
       "                  role                                           overview  \\\n",
       "0      Ph.D. Candidate  Aaliya is a Ph.D. Candidate working under the ...   \n",
       "1  Associate Professor  <p><strong>Ehsan Abadi, PhD</strong> is an ima...   \n",
       "2                 None                                               None   \n",
       "3                 None                                               None   \n",
       "4   Professor Emeritus  My research interests include the clinical stu...   \n",
       "\n",
       "   pub_count latest_pub_date  \n",
       "0          0            None  \n",
       "1        170      2025-11-01  \n",
       "2          1      2025-02-01  \n",
       "3          0            None  \n",
       "4        585      2025-02-01  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# generate table one: researcher table \n",
    "import pandas as pd\n",
    "\n",
    "rows = []\n",
    "\n",
    "for person in data:   # data is your list of 11547 researchers\n",
    "\n",
    "    # 1. unique researcher id\n",
    "    rid = person.get(\"unique_id\")\n",
    "\n",
    "    # 2. name fields\n",
    "    first = person.get(\"firstName\")\n",
    "    last = person.get(\"lastName\")\n",
    "    full = f\"{first} {last}\".strip() if first or last else None\n",
    "\n",
    "    # 3. email\n",
    "    email = person.get(\"email\")\n",
    "\n",
    "    # 4. primary appointment title\n",
    "    pa = person.get(\"primaryAppointment\")\n",
    "    if isinstance(pa, dict):\n",
    "        title = pa.get(\"title\")\n",
    "    else:\n",
    "        title = None\n",
    "\n",
    "    # 5. department_raw from your earlier extraction\n",
    "    dept = person.get(\"department_raw\")\n",
    "\n",
    "    # 6. role (you already extracted earlier)\n",
    "    role = person.get(\"role\")\n",
    "\n",
    "    # 7. publication count\n",
    "    pubs = person.get(\"publications\", {})\n",
    "    pub_count = pubs.get(\"count\", 0) if isinstance(pubs, dict) else 0\n",
    "\n",
    "     # 8. latest publication date (take max over all publicationDate.date)\n",
    "    latest_pub = None\n",
    "    if isinstance(pubs, dict):\n",
    "        results = pubs.get(\"results\", [])\n",
    "        dates = []\n",
    "        for p in results:\n",
    "            pub_info = p.get(\"publication\", {}) or {}\n",
    "            pub_date_obj = pub_info.get(\"publicationDate\") or {}\n",
    "            date_str = pub_date_obj.get(\"date\")   # e.g. \"2025-02-01\"\n",
    "            if date_str:\n",
    "                dates.append(date_str)\n",
    "        if dates:\n",
    "            latest_pub = max(dates)\n",
    "\n",
    "    \n",
    "\n",
    "    #9. overview\n",
    "    overview = person.get(\"overview\")\n",
    "\n",
    "    # add row\n",
    "    rows.append({\n",
    "        \"researcher_id\": rid,\n",
    "        \"first_name\": first,\n",
    "        \"last_name\": last,\n",
    "        \"full_name\": full,\n",
    "        \"email\": email,\n",
    "        \"title\": title,\n",
    "        \"department_raw\": dept,\n",
    "        \"role\": role,\n",
    "        \"overview\": overview,\n",
    "        \"pub_count\": pub_count,\n",
    "        \"latest_pub_date\": latest_pub,\n",
    "    \n",
    "    })\n",
    "\n",
    "# convert to DataFrame\n",
    "researcher_df = pd.DataFrame(rows)\n",
    "\n",
    "researcher_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "229b65d3-06a4-4975-90b5-a5bca5f3c777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original rows: 481634\n",
      "After de-duplicating by (researcher_id, pub_title_clean): 463014\n"
     ]
    }
   ],
   "source": [
    "#generate table two: publication table \n",
    "# step one - original publication-author  （include duplicated author-pub relationships)\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def clean_title(title):\n",
    "    \"\"\"\n",
    "    Clean publication titles for consistency.\n",
    "    - Lowercase\n",
    "    - Strip whitespace\n",
    "    - Normalize unicode\n",
    "    - Remove repeated spaces\n",
    "    - Remove trailing punctuation (.,;:)\n",
    "    \"\"\"\n",
    "\n",
    "    if title is None:\n",
    "        return None\n",
    "    \n",
    "    # Normalize unicode (fix accents, weird characters)\n",
    "    title = unicodedata.normalize(\"NFKC\", title)\n",
    "\n",
    "    # Lowercase\n",
    "    title = title.lower()\n",
    "\n",
    "    # Remove extra spaces\n",
    "    title = re.sub(r\"\\s+\", \" \", title).strip()\n",
    "\n",
    "    # Remove trailing punctuation such as \".\", \";\", \":\" \n",
    "    title = re.sub(r\"[.;:]+$\", \"\", title)\n",
    "\n",
    "    return title\n",
    "\n",
    "\n",
    "pub_rows = []\n",
    "\n",
    "for person in data:  # data is your list of researchers\n",
    "\n",
    "    # researcher basic info\n",
    "    researcher_id = person.get(\"unique_id\")\n",
    "    first = person.get(\"firstName\")\n",
    "    last = person.get(\"lastName\")\n",
    "    full_name = f\"{first} {last}\".strip() if first or last else None\n",
    "    \n",
    "    # publications block\n",
    "    pubs = person.get(\"publications\", {})\n",
    "    if isinstance(pubs, dict):\n",
    "        results = pubs.get(\"results\", [])\n",
    "    else:\n",
    "        results = []\n",
    "    \n",
    "    # loop over each publication for this researcher\n",
    "    for p in results:\n",
    "        raw_pub_id = p.get(\"id\")\n",
    "        pub_info = p.get(\"publication\", {}) or {}\n",
    "        \n",
    "        pub_title_raw = pub_info.get(\"title\")\n",
    "        pub_title_clean = clean_title(pub_title_raw)\n",
    "\n",
    "        pub_abstract = pub_info.get(\"abstract\")\n",
    "        pub_doi = pub_info.get(\"doi\")\n",
    "        \n",
    "        all_authors_obj = pub_info.get(\"allAuthors\", {}) or {}\n",
    "        all_authors = all_authors_obj.get(\"fullList\")\n",
    "\n",
    "        # extract publication date\n",
    "        date_obj = pub_info.get(\"publicationDate\", {}) or {}\n",
    "        pub_date = date_obj.get(\"date\")\n",
    "\n",
    "        # append row\n",
    "        pub_rows.append({\n",
    "            \"raw_pub_id\": raw_pub_id,\n",
    "            \"researcher_id\": researcher_id,\n",
    "            \"full_name\": full_name,\n",
    "            \"pub_title_raw\": pub_title_raw,\n",
    "            \"pub_title_clean\": pub_title_clean,\n",
    "            \"pub_abstract\": pub_abstract,\n",
    "            \"pub_doi\": pub_doi,\n",
    "            \"all_authors\": all_authors,\n",
    "            \"pub_date\": pub_date\n",
    "        })\n",
    "\n",
    "# build the publication DataFrame\n",
    "raw_pub_df = pd.DataFrame(pub_rows)\n",
    "\n",
    "### step 2 deduplicate author-pub rows \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Make a copy so we don't modify raw_pub_df in place\n",
    "dedup_pub_df = raw_pub_df.copy()\n",
    "\n",
    "# Parse pub_date as datetime (invalid or missing dates become NaT)\n",
    "dedup_pub_df[\"pub_date_parsed\"] = pd.to_datetime(\n",
    "    dedup_pub_df[\"pub_date\"], errors=\"coerce\"\n",
    ")\n",
    "\n",
    "# Helper flag: True if this row has a valid date, False otherwise\n",
    "dedup_pub_df[\"has_date\"] = dedup_pub_df[\"pub_date_parsed\"].notna()\n",
    "\n",
    "# Sort so that:\n",
    "# - same (researcher_id, pub_title_clean) group\n",
    "# - rows with a date come BEFORE rows without date\n",
    "# - among rows with dates, the LATEST date comes first\n",
    "dedup_pub_df = dedup_pub_df.sort_values(\n",
    "    by=[\"researcher_id\", \"pub_title_clean\", \"has_date\", \"pub_date_parsed\"],\n",
    "    ascending=[True, True, False, False]  # keep best row at the top of each group\n",
    ")\n",
    "\n",
    "# Now drop duplicates: keep the first row in each (researcher_id, pub_title_clean) group\n",
    "dedup_pub_df = dedup_pub_df.drop_duplicates(\n",
    "    subset=[\"researcher_id\", \"pub_title_clean\"],\n",
    "    keep=\"first\"\n",
    ")\n",
    "\n",
    "# Drop helper columns\n",
    "dedup_pub_df = dedup_pub_df.drop(columns=[\"pub_date_parsed\", \"has_date\"])\n",
    "\n",
    "# Optional: check how many rows were removed\n",
    "print(\"Original rows:\", raw_pub_df.shape[0])\n",
    "print(\"After de-duplicating by (researcher_id, pub_title_clean):\", dedup_pub_df.shape[0])\n",
    "\n",
    "raw_pub_df = dedup_pub_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0838da6a-0abb-49b7-91ff-73c554ae846e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1529f9-e9c6-4620-8bee-5dce56cf8fd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaae9d2c-fbea-4321-9e1e-ae6116a7064a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b68faf9-9487-4341-8b55-a63160bdd451",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     26\u001b[39m duke_authors = \u001b[33m\"\u001b[39m\u001b[33m; \u001b[39m\u001b[33m\"\u001b[39m.join(group[\u001b[33m\"\u001b[39m\u001b[33mfull_name\u001b[39m\u001b[33m\"\u001b[39m].dropna().unique())\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# All raw_pub_id values in this group\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m raw_pub_ids = \u001b[38;5;28mlist\u001b[39m(\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mraw_pub_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Representative original (raw) title\u001b[39;00m\n\u001b[32m     32\u001b[39m pub_title_raw = group[\u001b[33m\"\u001b[39m\u001b[33mpub_title_raw\u001b[39m\u001b[33m\"\u001b[39m].iloc[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alejandro\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\series.py:2416\u001b[39m, in \u001b[36mSeries.unique\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2353\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34munique\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> ArrayLike:  \u001b[38;5;66;03m# pylint: disable=useless-parent-delegation\u001b[39;00m\n\u001b[32m   2354\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2355\u001b[39m \u001b[33;03m    Return unique values of Series object.\u001b[39;00m\n\u001b[32m   2356\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   2414\u001b[39m \u001b[33;03m    Categories (3, object): ['a' < 'b' < 'c']\u001b[39;00m\n\u001b[32m   2415\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2416\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alejandro\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\base.py:1029\u001b[39m, in \u001b[36mIndexOpsMixin.unique\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1027\u001b[39m     result = values.unique()\n\u001b[32m   1028\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1029\u001b[39m     result = \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43munique1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1030\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alejandro\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\algorithms.py:401\u001b[39m, in \u001b[36munique\u001b[39m\u001b[34m(values)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34munique\u001b[39m(values):\n\u001b[32m    308\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    309\u001b[39m \u001b[33;03m    Return unique values based on a hash table.\u001b[39;00m\n\u001b[32m    310\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    399\u001b[39m \u001b[33;03m    array([('a', 'b'), ('b', 'a'), ('a', 'c')], dtype=object)\u001b[39;00m\n\u001b[32m    400\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43munique_with_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alejandro\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\algorithms.py:440\u001b[39m, in \u001b[36munique_with_mask\u001b[39m\u001b[34m(values, mask)\u001b[39m\n\u001b[32m    438\u001b[39m table = hashtable(\u001b[38;5;28mlen\u001b[39m(values))\n\u001b[32m    439\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m     uniques = \u001b[43mtable\u001b[49m\u001b[43m.\u001b[49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    441\u001b[39m     uniques = _reconstruct_data(uniques, original.dtype, original)\n\u001b[32m    442\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m uniques\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#generate table two: publication table \n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Use global random seed for reproducible IDs\n",
    "random.seed(999999)\n",
    "\n",
    "def stable_id():\n",
    "    # Generate a reproducible pseudo-random hex string ID\n",
    "    return hex(random.getrandbits(64))[2:]\n",
    "\n",
    "\n",
    "clean_pub = raw_pub_df.dropna(subset=[\"pub_title_clean\"]).copy()\n",
    "clean_pub = clean_pub[clean_pub[\"pub_title_clean\"].astype(str).str.strip() != \"\"]\n",
    "\n",
    "grouped = clean_pub.groupby(\"pub_title_clean\")\n",
    "\n",
    "new_pub_rows = []\n",
    "\n",
    "for title_clean, group in grouped:\n",
    "    \n",
    "    # Stable ID for this publication (one per cleaned title)\n",
    "    pub_unique_id = stable_id()\n",
    "    \n",
    "    # Unique Duke authors in this group\n",
    "    duke_authors = \"; \".join(group[\"full_name\"].dropna().unique())\n",
    "    \n",
    "    # All raw_pub_id values in this group\n",
    "    raw_pub_ids = list(group[\"raw_pub_id\"].dropna().unique())\n",
    "    \n",
    "    # Representative original (raw) title\n",
    "    pub_title_raw = group[\"pub_title_raw\"].iloc[0]\n",
    "    \n",
    "    # First non-null values for abstract, doi, date\n",
    "    pub_abstract = group[\"pub_abstract\"].iloc[0]\n",
    "    pub_doi = group[\"pub_doi\"].iloc[0]\n",
    "    pub_date = group[\"pub_date\"].iloc[0]\n",
    "    \n",
    "    new_pub_rows.append({\n",
    "        \"pub_unique_id\": pub_unique_id,\n",
    "        \"pub_title_raw\": pub_title_raw,\n",
    "        \"pub_title_clean\": title_clean,\n",
    "        \"duke_authors\": duke_authors,\n",
    "        \"raw_pub_ids\": raw_pub_ids,\n",
    "        \"pub_abstract\": pub_abstract,\n",
    "        \"pub_doi\": pub_doi,\n",
    "        \"pub_date\": pub_date\n",
    "    })\n",
    "\n",
    "publication_clean_df = pd.DataFrame(new_pub_rows)\n",
    "publication_clean_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b15e6e-60b6-4972-a6f0-e48b04deabe3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ba730f-a0c7-40de-ab6d-8955aaf405a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create coauthorship table \n",
    "# step one (create a table one row per pub_id + researcher pairs (not dedepulicated) \n",
    "from itertools import combinations\n",
    "import pandas as pd\n",
    "\n",
    "author_pub = raw_pub_df[[\n",
    "    \"pub_title_clean\",\n",
    "    \"researcher_id\",\n",
    "    \"full_name\"\n",
    "]].dropna(subset=[\"pub_title_clean\", \"researcher_id\"])\n",
    "\n",
    "author_pub = author_pub.merge(\n",
    "    publication_clean_df[[\"pub_unique_id\", \"pub_title_clean\"]],\n",
    "    on=\"pub_title_clean\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "print(\"Total author–pub rows:\", len(author_pub))\n",
    "print(\"Unique publication_unique_id:\", author_pub[\"pub_unique_id\"].nunique())\n",
    "\n",
    "pairs = []\n",
    "\n",
    "for uid, group in author_pub.groupby(\"pub_unique_id\"):\n",
    "    authors = group.to_dict(\"records\")\n",
    "\n",
    "    if len(authors) < 2:\n",
    "        continue\n",
    "\n",
    "    for a, b in combinations(authors, 2):\n",
    "\n",
    "        if a[\"researcher_id\"] < b[\"researcher_id\"]:\n",
    "            left, right = a, b\n",
    "        else:\n",
    "            left, right = b, a\n",
    "\n",
    "        if left[\"researcher_id\"] == right[\"researcher_id\"]:\n",
    "            continue\n",
    "\n",
    "        pairs.append({\n",
    "            \"pub_unique_id\": uid,\n",
    "            \"pub_title_cleaned\":pub_title_clean,\n",
    "            \"researcher_A_id\": left[\"researcher_id\"],\n",
    "            \"researcher_B_id\": right[\"researcher_id\"],\n",
    "            \"researcher_A_name\": left[\"full_name\"],\n",
    "            \"researcher_B_name\": right[\"full_name\"]\n",
    "        })\n",
    "\n",
    "coauthor_pairs = pd.DataFrame(pairs)\n",
    "\n",
    "print(\"Total raw coauthor pairs:\", len(coauthor_pairs))\n",
    "coauthor_pairs.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b335be4-7b6d-4586-a71c-c1fcd1b32c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "coauthor_pairs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db56f72-64e9-45ef-bdd1-dbe65cc06f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create coauthorship table\n",
    "# step two (created the deduplicated pairs with their joint pub counts)\n",
    "coauthor_pairs_final = (\n",
    "    coauthor_pairs\n",
    "        .groupby(\n",
    "            [\"researcher_A_id\", \"researcher_B_id\"],\n",
    "            as_index=False\n",
    "        )\n",
    "        .agg({\n",
    "            \"pub_unique_id\": \"nunique\",          \n",
    "            \"researcher_A_name\": \"first\",        \n",
    "            \"researcher_B_name\": \"first\"\n",
    "        })\n",
    "        .rename(columns={\"pub_unique_id\": \"joint_publications\"})\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391eb7c1-35c6-4244-be5b-98f8a39a5603",
   "metadata": {},
   "outputs": [],
   "source": [
    "coauthor_pairs_final.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c6251e-22c2-421c-84f4-55a7522fcb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyreadr\n",
    "import pyreadr\n",
    "\n",
    "pyreadr.write_rds(\"researcher_df.rds\", researcher_df)\n",
    "pyreadr.write_rds(\"publication_author_per_row\", raw_pub_df)\n",
    "pyreadr.write_rds(\"publication_clean_df.rds\", publication_clean_df)\n",
    "pyreadr.write_rds(\"coauthor_pairs_final.rds\", coauthor_pairs_final)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
